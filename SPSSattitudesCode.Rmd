---
title: "SPSS Attitudes Measure Validation"
author: "Carmen Dang"
date: "13/07/2020"
output: html_document
        md_keep = TRUE
---

```{r Loading Packages, echo = FALSE}
library(readr)
library(haven)
library(tidyverse)
library(lavaan)
library(psych)
library(mirt)
library(rsample)
```

```{r Uploading data and cleaning}
# Uploading raw data
full.data <- readxl::read_xlsx("spssdata.xlsx", col_names = TRUE)
# Remove empty rows
full.data <- full.data[1:181, ]

# Selecting only SPSS-related columns
  # Do not select StudentIDE col bc it'd be included with EFA/CFA
  # Num of rows corresponds to StudentIDE
spss.data <- full.data %>% 
             select(SPSS1E,
                    SPSS2E,
                    SPSS3E,
                    SPSS4E,
                    SPSS5E,
                    SPSS6E,
                    SPSS7E,
                    SPSS8E,
                    SPSS9E,
                    SPSS10E)

# Reverse code SPSS items 2, 3, 10
spss.data$SPSS2E <- car::recode(spss.data$SPSS2E, "1 = 5; 2 = 4; 3 = 3; 4 = 2; 5 = 1")
spss.data$SPSS3E <- car::recode(spss.data$SPSS3E, "1 = 5; 2 = 4; 3 = 3; 4 = 2; 5 = 1")
spss.data$SPSS10E <- car::recode(spss.data$SPSS10E, "1 = 5; 2 = 4; 3 = 3; 4 = 2; 5 = 1")
```

# Questionnaire Info
<!-- 1 Always True 2 Mostly True	3 Sometimes True 4 Rarely True 5 Never True -->

<!-- SPSS1E	SPSS helped me understand statistics concepts better. -->
<!-- SPSS2E	SPSS has not helped me do well in the course. -->
<!-- SPSS3E	SPSS has not helped me integrate and apply class concepts. -->
<!-- SPSS4E	SPSS has helped me become more comfortable with statistics. -->
<!-- SPSS5E	SPSS provided me with a good application experience. -->
<!-- SPSS6E	SPSS has been convenient for me to do assignments. -->
<!-- SPSS7E	I think the skills Iâ€™m learning with SPSS will be useful outside of class. -->
<!-- SPSS8E	SPSS helps me get involved in learning class material. -->
<!-- SPSS9E	SPSS has helped me gain confidence in the course material. -->
<!-- SPSS10E	SPSS has not helped me make more sense out of the material. -->

# Descriptives

```{r Descriptive Stats}
describe(spss.data)

# Contingency table of the counts
table(spss.data$SPSS1E)
table(spss.data$SPSS2E)
table(spss.data$SPSS3E)
table(spss.data$SPSS4E)
table(spss.data$SPSS5E)
table(spss.data$SPSS6E)
table(spss.data$SPSS7E)
table(spss.data$SPSS8E)
table(spss.data$SPSS9E)
table(spss.data$SPSS10E)

## Missing Data Calculations ##
table(is.na(spss.data))

# FALSE  TRUE 
#  1807     3

# 3 / (3 + 1807) = 0.001657459
# 0.001657459 * 100 = 0.1657459

## Less than 1% missing data, proceeding with complete case analyses
```
```{r Scatterplot matrix}
car::scatterplotMatrix(spss.data, smooth = F, regLine = F, col = 'black')
```
## Correlations

```{r polychoric}
psych::polychoric(spss.data) # wants numeric data

# 44 cells were adjusted for 0 values using the correction for continuity. Examine your data carefully.Call: psych::polychoric(x = spss.data)
# Polychoric correlations 
#         SPSS1E SPSS2 SPSS3 SPSS4 SPSS5 SPSS6 SPSS7 SPSS8 SPSS9 SPSS10
# SPSS1E  1.00                                                         
# SPSS2E  0.37   1.00                                                  
# SPSS3E  0.34   0.66  1.00                                            
# SPSS4E  0.76   0.30  0.35  1.00                                      
# SPSS5E  0.67   0.34  0.27  0.66  1.00                                
# SPSS6E  0.52   0.27  0.23  0.52  0.56  1.00                          
# SPSS7E  0.52   0.25  0.14  0.53  0.51  0.44  1.00                    
# SPSS8E  0.71   0.32  0.35  0.66  0.67  0.57  0.54  1.00              
# SPSS9E  0.74   0.29  0.34  0.76  0.65  0.58  0.56  0.71  1.00        
# SPSS10E 0.26   0.49  0.50  0.23  0.13  0.06  0.03  0.23  0.12  1.00  
# 
#  with tau of 
#            1     2      3    4
# SPSS1E  -1.7 -0.99 -0.084 0.97
# SPSS2E  -1.8 -0.89 -0.361 0.66
# SPSS3E  -1.6 -0.88 -0.182 0.90
# SPSS4E  -1.8 -0.89  0.035 1.20
# SPSS5E  -1.8 -1.19 -0.239 0.66
# SPSS6E  -1.4 -0.91 -0.090 0.64
# SPSS7E  -1.6 -0.59  0.104 0.87
# SPSS8E  -1.9 -1.14 -0.132 0.93
# SPSS9E  -1.8 -0.95  0.062 0.99
# SPSS10E -1.6 -0.77 -0.021 0.91

####
# Used polychoric bc Likert data
# All Qs are more correlated w eachother than they are with 2,3,10. But 2,3, and 10 are more correlated to each other than other Qs.
####
```
# MAP / PA

```{r}
VSS(spss.data, fm = 'minres', cor = 'poly', plot = F)

# Very Simple Structure
# Call: vss(x = x, n = n, rotate = rotate, diagonal = diagonal, fm = fm, 
#     n.obs = n.obs, plot = plot, title = title, use = use, cor = cor)
# VSS complexity 1 achieves a maximimum of 0.85  with  1  factors
# VSS complexity 2 achieves a maximimum of 0.94  with  2  factors
# 
# The Velicer MAP achieves a minimum of 0.04  with  2  factors 
# BIC achieves a minimum of  NA  with  2  factors
# Sample Size adjusted BIC achieves a minimum of  NA  with  3  factors
# 
# Statistics by number of factors 

fa.parallel(spss.data, fm = 'minres', cor = 'poly', fa ='both', n.iter=100)

# Parallel analysis suggests that the number of factors =  2  and the number of components =  2 

####
# Both MAP and PA suggest 2F
# PA should be interpreted w caution for polychoric
####

# Running 1F, 2F and 3F model (i.e. 1 above and 1 below suggested num. of factors) next to help determine which model is best
```

# EFAs

## 1F

```{r}
fa(r = spss.data, fm = 'minres', rotate = "oblimin", cor = 'poly', nfactors = 1)

# 44 cells were adjusted for 0 values using the correction for continuity. Examine your data carefully.Factor Analysis using method =  minres
# Call: fa(r = spss.data, nfactors = 1, rotate = "oblimin", fm = "minres", 
#     cor = "poly")
# Standardized loadings (pattern matrix) based upon correlation matrix
# 
#                 MR1
# SS loadings    4.74
# Proportion Var 0.47
# 
# Mean item complexity =  1
# Test of the hypothesis that 1 factor is sufficient.
# 
# The degrees of freedom for the null model are  45  and the objective function was  5.86 with Chi Square of  1030.98
# The degrees of freedom for the model are 35  and the objective function was  1.03 
# 
# The root mean square of the residuals (RMSR) is  0.12 
# The df corrected root mean square of the residuals is  0.13 
# 
# The harmonic number of observations is  180 with the empirical chi square  218.15  with prob <  2.5e-28 
# The total number of observations was  181  with Likelihood Chi Square =  179.94  with prob <  2.1e-21 
# 
# Tucker Lewis Index of factoring reliability =  0.81
# RMSEA index =  0.151  and the 90 % confidence intervals are  0.13 0.174
# BIC =  -2.01
# Fit based upon off diagonal values = 0.94
# Measures of factor score adequacy             
#                                                    MR1
# Correlation of (regression) scores with factors   0.96
# Multiple R square of scores with factors          0.93
# Minimum correlation of possible factor scores     0.86
# 
#         MR1       h2          u2    com
# SPSS1E	0.85	0.72856429	0.2714357	1     
# SPSS2E	0.47	0.21794717	0.7820528	1     *
# SPSS3E	0.45	0.20643367	0.7935663	1     *
# SPSS4E	0.83	0.69670807	0.3032919	1
# SPSS5E	0.78	0.60983397	0.3901660	1
# SPSS6E	0.65	0.42275333	0.5772467	1
# SPSS7E	0.61	0.37574147	0.6242585	1
# SPSS8E	0.83	0.68331094	0.3166891	1
# SPSS9E	0.85	0.71463561	0.2853644	1
# SPSS10E	0.28	0.08033225	0.9196677	1     **

####
# RMSR = 0.12 *BAD*
# Prop. var explained = 0.47 
# SPSS10E *BAD* factor loading (<.4) and communality (0.08)
# SPSS2E and SPSS3E factor loading almost <.4 and communality almost <.2
#
# Based on model fit (RMSR), 1F sucks
####
```
## 2F

```{r}
fa(r = spss.data, fm = 'minres', cor = 'poly', nfactors = 2)

# 44 cells were adjusted for 0 values using the correction for continuity. Examine your data carefully.Factor Analysis using method =  minres
# Call: fa(r = spss.data, nfactors = 2, fm = "minres", cor = "poly")
# Standardized loadings (pattern matrix) based upon correlation matrix
# 
#                        MR1  MR2
# SS loadings           4.35 1.71
# Proportion Var        0.43 0.17
# Cumulative Var        0.43 0.61
# Proportion Explained  0.72 0.28
# Cumulative Proportion 0.72 1.00
# 
#  With factor correlations of 
#      MR1  MR2
# MR1 1.00 0.44
# MR2 0.44 1.00
# 
# Mean item complexity =  1
# Test of the hypothesis that 2 factors are sufficient.
# 
# The degrees of freedom for the null model are  45  and the objective function was  5.86 with Chi Square of  1030.98
# The degrees of freedom for the model are 26  and the objective function was  0.25 
# 
# The root mean square of the residuals (RMSR) is  0.03 
# The df corrected root mean square of the residuals is  0.04 
# 
# The harmonic number of observations is  180 with the empirical chi square  13.09  with prob <  0.98 
# The total number of observations was  181  with Likelihood Chi Square =  44.2  with prob <  0.014 
# 
# Tucker Lewis Index of factoring reliability =  0.968
# RMSEA index =  0.062  and the 90 % confidence intervals are  0.028 0.093
# BIC =  -90.96
# Fit based upon off diagonal values = 1
# Measures of factor score adequacy             
#                                                    MR1  MR2
# Correlation of (regression) scores with factors   0.96 0.91
# Multiple R square of scores with factors          0.93 0.82
# Minimum correlation of possible factor scores     0.86 0.65
# 
#         MR1   MR2       h2        u2    com
# SPSS1E	0.81	0.08	0.7236662	0.2763338	1.019276
# SPSS2E	0.06	0.76	0.6178306	0.3821694	1.012793
# SPSS3E	0.02	0.82	0.6827871	0.3172129	1.000986
# SPSS4E	0.82	0.04	0.7018363	0.2981637	1.004479
# SPSS5E	0.80	-0.01	0.6295613	0.3704387	1.000441
# SPSS6E	0.68	-0.04	0.4448130	0.5551870	1.006886
# SPSS7E	0.69	-0.11	0.4244661	0.5755339	1.054229
# SPSS8E	0.80	0.05	0.6842864	0.3157136	1.009224
# SPSS9E	0.88	-0.04	0.7508519	0.2491481	1.003669
# SPSS10E	-0.09	0.66	0.3975595	0.6024405	1.036934

####
# RMSR = 0.03 *WOW!* huge decrease by adding 1 more factor
# Prop. var explained = 0.61, 14% raw difference from 1F model
# No poor factor loadings or low communalities
# Column and row parsimony is pretty amazing
# Notice that all negatively worded items load onto factor 2 & all positively worded items load onto factor 1
# 2F prob wins, but let's try 3F next anyways
####
```
## 3F

```{r}
fa(r = spss.data, fm = 'minres', cor = 'poly', nfactors = 3)

# 44 cells were adjusted for 0 values using the correction for continuity. Examine your data carefully.Factor Analysis using method =  minres
# Call: fa(r = spss.data, nfactors = 3, fm = "minres", cor = "poly")
# Standardized loadings (pattern matrix) based upon correlation matrix
# 
#                        MR1  MR2  MR3
# SS loadings           4.39 1.49 0.65
# Proportion Var        0.44 0.15 0.07
# Cumulative Var        0.44 0.59 0.65
# Proportion Explained  0.67 0.23 0.10
# Cumulative Proportion 0.67 0.90 1.00
# 
#  With factor correlations of 
#      MR1  MR2  MR3
# MR1 1.00 0.39 0.13
# MR2 0.39 1.00 0.37
# MR3 0.13 0.37 1.00
# 
# Mean item complexity =  1.2
# Test of the hypothesis that 3 factors are sufficient.
# 
# The degrees of freedom for the null model are  45  and the objective function was  5.86 with Chi Square of  1030.98
# The degrees of freedom for the model are 18  and the objective function was  0.14 
# 
# The root mean square of the residuals (RMSR) is  0.02 
# The df corrected root mean square of the residuals is  0.03 
# 
# The harmonic number of observations is  180 with the empirical chi square  5.49  with prob <  1 
# The total number of observations was  181  with Likelihood Chi Square =  24.9  with prob <  0.13 
# 
# Tucker Lewis Index of factoring reliability =  0.982
# RMSEA index =  0.046  and the 90 % confidence intervals are  0 0.086
# BIC =  -68.68
# Fit based upon off diagonal values = 1
# Measures of factor score adequacy             
#                                                    MR1 MR2  MR3
# Correlation of (regression) scores with factors   0.97   1 0.76
# Multiple R square of scores with factors          0.93   1 0.58
# Minimum correlation of possible factor scores     0.87   1 0.16
# 
#         MR1   MR2   MR3       h2        u2      com
# SPSS1E	0.84	-0.01	0.11	0.7403837	0.259616297	1.036000
# SPSS2E	-0.01	1.00	0.00	0.9956504	0.004349638	1.000068
# SPSS3E	0.15	0.47	0.36	0.5686618	0.431338226	2.084417
# SPSS4E	0.86	-0.07	0.12	0.7317213	0.268278681	1.055702
# SPSS5E	0.77	0.08	-0.11	0.6345384	0.365461572	1.067785
# SPSS6E	0.64	0.10	-0.18	0.4664426	0.533557360	1.205680
# SPSS7E	0.64	0.07	-0.22	0.4508917	0.549108279	1.270510
# SPSS8E	0.82	0.01	0.04	0.6870119	0.312988095	1.006366
# SPSS9E	0.88	-0.03	-0.02	0.7511585	0.248841506	1.003480
# SPSS10E	0.04	0.27	0.56	0.5062815	0.493718467	1.455608

####
# RMSR = 0.02 *MEH* only decreased by 0.01 after adding an additional factor - not worth it bc RMSR always decreases when adding an additional factor.
# Prop. var explained = 0.65, 4% raw difference from 2F model
# No low communalities, BUT
  # SPSS3E has a cross-loading
  # In general, column and row parsimony is not nearly as good as 2F model
  # A question: SPSS2E loads onto MR2 at 1.00(!)
# Concluding that 2F wins bc improvements in model fit isn't worth it & column and row parsimony worse than 2F model 
####
```
# 2F with rotations

<!-- The original with oblimin: -->

<!-- #         MR1   MR2       h2        u2    com -->
<!-- # SPSS1E	0.81	0.08	0.7236662	0.2763338	1.019276 -->
<!-- # SPSS2E	0.06	0.76	0.6178306	0.3821694	1.012793 -->
<!-- # SPSS3E	0.02	0.82	0.6827871	0.3172129	1.000986 -->
<!-- # SPSS4E	0.82	0.04	0.7018363	0.2981637	1.004479 -->
<!-- # SPSS5E	0.80	-0.01	0.6295613	0.3704387	1.000441 -->
<!-- # SPSS6E	0.68	-0.04	0.4448130	0.5551870	1.006886 -->
<!-- # SPSS7E	0.69	-0.11	0.4244661	0.5755339	1.054229 -->
<!-- # SPSS8E	0.80	0.05	0.6842864	0.3157136	1.009224 -->
<!-- # SPSS9E	0.88	-0.04	0.7508519	0.2491481	1.003669 -->
<!-- # SPSS10E	-0.09	0.66	0.3975595	0.6024405	1.036934 -->

```{r}
fa(r = spss.data, fm = 'minres', cor = 'poly', rotate = 'bentlerQ', nfactors = 2)

#          MR1   MR2       h2        u2    com
# SPSS1E	0.81	0.08	0.7236662	0.2763338	1.018186
# SPSS2E	0.05	0.76	0.6178306	0.3821694	1.010090
# SPSS3E	0.01	0.82	0.6827871	0.3172129	1.000358
# SPSS4E	0.82	0.04	0.7018363	0.2981637	1.003928
# SPSS5E	0.80	-0.01	0.6295613	0.3704387	1.000650
# SPSS6E	0.68	-0.04	0.4448130	0.5551870	1.007671
# SPSS7E	0.70	-0.12	0.4244661	0.5755339	1.056426
# SPSS8E	0.80	0.05	0.6842864	0.3157136	1.008445
# SPSS9E	0.88	-0.04	0.7508519	0.2491481	1.004244
# SPSS10E	-0.10	0.67	0.3975595	0.6024405	1.041620

####
# Very close to begin identifical to oblimin
####
```

## GeominQ

```{r}
fa(r = spss.data, fm = 'minres', cor = 'poly', rotate = 'geominQ', nfactors = 2)

#           MR1   MR2       h2        u2    com
# SPSS1E	0.81	0.09	0.7236662	0.2763338	1.022965
# SPSS2E	0.06	0.76	0.6178306	0.3821694	1.014742
# SPSS3E	0.02	0.82	0.6827871	0.3172129	1.001591
# SPSS4E	0.82	0.05	0.7018363	0.2981637	1.006339
# SPSS5E	0.80	0.00	0.6295613	0.3704387	1.000070
# SPSS6E	0.68	-0.03	0.4448130	0.5551870	1.004967
# SPSS7E	0.69	-0.11	0.4244661	0.5755339	1.048740
# SPSS8E	0.80	0.06	0.6842864	0.3157136	1.011826
# SPSS9E	0.88	-0.03	0.7508519	0.2491481	1.002307
# SPSS10E	-0.09	0.66	0.3975595	0.6024405	1.033722

####
# Identical to oblimin
####
```

## Quartimin

```{r}
fa(r = spss.data, fm = 'minres', cor = 'poly', rotate = "quartimin", nfactors = 2)

#          MR1   MR2       h2        u2    com
# SPSS1E	0.81	0.08	0.7236662	0.2763338	1.019276
# SPSS2E	0.06	0.76	0.6178306	0.3821694	1.012793
# SPSS3E	0.02	0.82	0.6827871	0.3172129	1.000986
# SPSS4E	0.82	0.04	0.7018363	0.2981637	1.004479
# SPSS5E	0.80	-0.01	0.6295613	0.3704387	1.000441
# SPSS6E	0.68	-0.04	0.4448130	0.5551870	1.006886
# SPSS7E	0.69	-0.11	0.4244661	0.5755339	1.054229
# SPSS8E	0.80	0.05	0.6842864	0.3157136	1.009224
# SPSS9E	0.88	-0.04	0.7508519	0.2491481	1.003669
# SPSS10E	-0.09	0.66	0.3975595	0.6024405	1.036934

####
# Identical to oblimin
####
```


## Promax

```{r}
fa(r = spss.data, fm = 'minres', cor = 'poly', rotate = "Promax", nfactors = 2)

#         MR1   MR2       h2        u2    com
# SPSS1E	0.81	0.09	0.7236662	0.2763338	1.025101
# SPSS2E	0.09	0.75	0.6178306	0.3821694	1.026425
# SPSS3E	0.05	0.81	0.6827871	0.3172129	1.006460
# SPSS4E	0.82	0.05	0.7018363	0.2981637	1.007655
# SPSS5E	0.79	0.00	0.6295613	0.3704387	1.000000
# SPSS6E	0.68	-0.03	0.4448130	0.5551870	1.003746
# SPSS7E	0.69	-0.10	0.4244661	0.5755339	1.044521
# SPSS8E	0.80	0.07	0.6842864	0.3157136	1.013513
# SPSS9E	0.88	-0.02	0.7508519	0.2491481	1.001514
# SPSS10E	-0.07	0.65	0.3975595	0.6024405	1.021111

####
# Not very differnt from oblimin, if anything - .01 worse than oblimin
####
```
I quickly ran the remaining oblique rotations mentioned in the 'fa' function's documentation. They're all nearly identical to oblimin as the ones above. 

# Assumptions

1. MV normality - N/A because we are using minres/ULS and not ML
2. Multicollinearity - not violated if the model converges (which it does)
3. Linearity - N/A bc polychoric correlations are used 
